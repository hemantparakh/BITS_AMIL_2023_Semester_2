{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "2BSjbZMcafdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-21wsv1wamG4",
        "outputId": "58726893-9682-4a3a-f0ee-60287589676c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vguRNDKHoSi6",
        "outputId": "750fd78a-7361-4561-bf51-a37dfa932328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the pdf file"
      ],
      "metadata": {
        "id": "gXTmnQOrr9fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text = \"Those Who Are Resilient Stay In The Game Longer. On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche. Challenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don’t have the answers. I can’t tell you what the right course of action is; only you will know. However, it’s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it’s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\"\n",
        "\n",
        "import pdfplumber\n",
        "\n",
        "# Path to the PDF file\n",
        "pdf_file_path = '/content/Paytm secures finance ministry approval for investment in payment services business - Times of India.pdf'\n",
        "\n",
        "# Open the PDF file\n",
        "with pdfplumber.open(pdf_file_path) as pdf:\n",
        "    # Initialize a variable to store the extracted text\n",
        "    text = ''\n",
        "\n",
        "    # Iterate through the pages and extract text\n",
        "    for page in pdf.pages:\n",
        "        text += page.extract_text()\n",
        "\n",
        "# Display the extracted text\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "L587J_1aeyOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decac329-52cb-445d-fefb-cbac9a8d466d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printed from\n",
            "Paytm secures finance ministry approval for\n",
            "investment in payment services business\n",
            "TIMESOFINDIA.COM | Aug 28, 2024, 08.46 PM IST\n",
            "Paytm, a prominent fintech company, has secured approval from\n",
            "the finance ministry to invest in its payment services business, the\n",
            "company announced on Wednesday.\n",
            "The company, officially known as One 97 Communications, has\n",
            "been under the watchful eye of banking regulator and financial\n",
            "crime-fighting agency following the central bank's directive in\n",
            "January to shut down its payments bank.\n",
            "With the recent approval, Paytm plans to submit a new\n",
            "application to the ministry to regain the license for its payments\n",
            "services business. The company said that Paytm Payment\n",
            "Services will continue to provide online payment aggregation\n",
            "services to existing partners.\n",
            "Although Paytm did not disclose the specifics of the approved investment, Reuters reported in July, based on\n",
            "information from a senior finance ministry official, that the company had obtained approval for a 500 million\n",
            "rupee (approximately $6 million) investment in its payments division.\n",
            "Paytm Payment Services, a significant component of the fintech firm's operations, contributed a quarter of its\n",
            "consolidated revenue in the fiscal year that ended in March 2023.\n",
            "In July, Vivek Joshi, financial services secretary, said that the company could approach India's central bank toapply for a payment aggregator license, which the bank would then assess.\n",
            "On the day of the announcement, Paytm's shares closed 1.3% lower. Since the central bank's order to wind down\n",
            "the payment bank in January, the company's shares have declined by more than 29%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenize the sentences"
      ],
      "metadata": {
        "id": "NmgYbBb-ahQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPFMLXOsBzeU"
      },
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(text) # NLTK function\n",
        "total_documents = len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create the Frequency matrix of the words in each sentence."
      ],
      "metadata": {
        "id": "n6sRM_viapLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopWords:\n",
        "                continue\n",
        "\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return frequency_matrix"
      ],
      "metadata": {
        "id": "qo8JrplOan2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Calculate TermFrequency and generate a matrix\n",
        "\n",
        "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)"
      ],
      "metadata": {
        "id": "zdKhaGd6ay0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix"
      ],
      "metadata": {
        "id": "T0YFvYc3an4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Creating a table for documents per words"
      ],
      "metadata": {
        "id": "BQdNgr_Ea8nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table"
      ],
      "metadata": {
        "id": "SEJ_F9Dran7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Calculate IDF and generate a matrix\n",
        "\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"
      ],
      "metadata": {
        "id": "CrrueEg-bBNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix"
      ],
      "metadata": {
        "id": "lAlxaVH4an-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Calculate TF-IDF and generate a matrix\n",
        "\n",
        "TF-IDF algorithm is made of 2 algorithms multiplied together."
      ],
      "metadata": {
        "id": "unL1hD_gbOKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix"
      ],
      "metadata": {
        "id": "CVnDVNApaoA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Score the sentences"
      ],
      "metadata": {
        "id": "p9fZ0-BtbalR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _score_sentences(tf_idf_matrix) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its word's TF\n",
        "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue"
      ],
      "metadata": {
        "id": "Bb4pvfCUaoDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Find the threshold"
      ],
      "metadata": {
        "id": "F-kUm_1WbhSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original summary_text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average"
      ],
      "metadata": {
        "id": "fpR4UmjTaoKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Generate the summary"
      ],
      "metadata": {
        "id": "2y8t4EnmbkZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "6rwrxDggbkg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution of the text summerization"
      ],
      "metadata": {
        "id": "wfLqrp4XbkoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "'''\n",
        "We already have a sentence tokenizer, so we just need\n",
        "to run the sent_tokenize() method to create the array of sentences.\n",
        "'''\n",
        "# 1 Sentence Tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "total_documents = len(sentences)\n",
        "#print(sentences)\n",
        "\n",
        "# 2 Create the Frequency matrix of the words in each sentence.\n",
        "freq_matrix = _create_frequency_matrix(sentences)\n",
        "#print(freq_matrix)\n",
        "\n",
        "'''\n",
        "Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
        "'''\n",
        "# 3 Calculate TermFrequency and generate a matrix\n",
        "tf_matrix = _create_tf_matrix(freq_matrix)\n",
        "#print(tf_matrix)\n",
        "\n",
        "# 4 creating table for documents per words\n",
        "count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
        "#print(count_doc_per_words)\n",
        "\n",
        "'''\n",
        "Inverse document frequency (IDF) is how unique or rare a word is.\n",
        "'''\n",
        "# 5 Calculate IDF and generate a matrix\n",
        "idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "#print(idf_matrix)\n",
        "\n",
        "# 6 Calculate TF-IDF and generate a matrix\n",
        "tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "#print(tf_idf_matrix)\n",
        "\n",
        "# 7 Important Algorithm: score the sentences\n",
        "sentence_scores = _score_sentences(tf_idf_matrix)\n",
        "#print(sentence_scores)\n",
        "\n",
        "# 8 Find the threshold\n",
        "threshold = _find_average_score(sentence_scores)\n",
        "#print(threshold)\n",
        "\n",
        "# 9 Important Algorithm: Generate the summary\n",
        "summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z25_R53Rbku7",
        "outputId": "ffc2059a-9372-4766-975d-9126d3a988ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The company said that Paytm Payment\n",
            "Services will continue to provide online payment aggregation\n",
            "services to existing partners. On the day of the announcement, Paytm's shares closed 1.3% lower.\n"
          ]
        }
      ]
    }
  ]
}